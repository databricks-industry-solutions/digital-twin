# =============================================================================
# DATABRICKS DIGITAL TWIN - BACKEND CONFIGURATION
# =============================================================================

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================
PORT=8080
SECRET_KEY=your-secret-key-change-in-production

# =============================================================================
# DATABRICKS CONFIGURATION
# =============================================================================
# Note: These should match your frontend configuration
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=dapixxxxxx
DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/your-warehouse-id
WAREHOUSE_ID=your-warehouse-id

# Database Schema
DATABRICKS_CATALOG=main
DATABRICKS_SCHEMA=your_schema
DATABRICKS_TABLE=bronze

# Advanced Table Configuration
SYNCED_TABLE_FULL_NAME=main.your_schema.synced_table
TRIPLE_TABLE_FULL_NAME=main.your_schema.latest_sensor_triples

# =============================================================================
# POSTGRESQL CONFIGURATION (Lakebase Integration)
# =============================================================================
# These settings are for Databricks Lakebase (PostgreSQL-compatible interface)
# Update these values with your actual Lakebase connection details

# Lakebase Database Configuration
PGDATABASE=main
PGUSER=your_username
PGHOST=your-workspace.cloud.databricks.com
PGPORT=5432
PGSSLMODE=require
PGAPPNAME=databricks-digital-twin

# OAuth token will be automatically managed via Databricks SDK
# No need to set PGPASSWORD as it uses OAuth tokens

# Token refresh interval (seconds) - refresh OAuth token every 15 minutes
PG_TOKEN_REFRESH_SECONDS=900

# =============================================================================
# LAKEBASE SPECIFIC CONFIGURATION
# =============================================================================
# These tables should match your Databricks setup
# They are used for both Lakebase (PostgreSQL) and direct Databricks SQL access

# Table containing the latest synchronized sensor data (for /latest endpoint)
LAKEBASE_SYNCED_TABLE=main.your_schema.synced_table

# Table containing RDF triples with timestamps (for point-in-time queries)
LAKEBASE_TRIPLE_TABLE=main.your_schema.latest_sensor_triples

# RDF models table (for storing user-created models)
RDF_MODELS_SCHEMA=main.your_schema
RDF_MODELS_TABLE=rdf_models

# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================
#
# 1. Copy this file to .env.local:
#    cp .env.example .env.local
#
# 2. Fill in your Databricks workspace details:
#    - DATABRICKS_HOST: Your workspace URL (e.g., https://e2-workspace.cloud.databricks.com)
#    - DATABRICKS_TOKEN: Your personal access token
#    - WAREHOUSE_ID: Your SQL warehouse ID
#
# 3. Configure Lakebase (PostgreSQL) integration:
#    - PGDATABASE: Usually 'main' for Unity Catalog
#    - PGUSER: Your Databricks username
#    - PGHOST: Same as DATABRICKS_HOST domain
#    - Ensure Lakebase is enabled in your Databricks workspace
#
# 4. Update table names to match your setup:
#    - SYNCED_TABLE_FULL_NAME: Table with latest sensor data
#    - TRIPLE_TABLE_FULL_NAME: Table with historical RDF triples
#
# 5. Start the server: python app.py
#
# NOTE: OAuth tokens are automatically managed - no PGPASSWORD needed
#