{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f065c8-359e-4ce7-a062-957f5ab762f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Create Sensor Bronze Table with Line Data Generator\n",
    "\n",
    "Requires Classic Compute Cluster - DBR >= 16.4 LTS or Serverless with environment version >= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0068bd63-c5cb-4af8-92ca-e87cc673e69c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "637047ec-881d-46bd-85b4-54e9a6a8ae36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/QuentinAmbard/mandrova (from -r ./line_data_generator/requirements.txt (line 1))\n  Cloning https://github.com/QuentinAmbard/mandrova to /tmp/pip-req-build-64nk7ilr\n  Running command git clone --filter=blob:none --quiet https://github.com/QuentinAmbard/mandrova /tmp/pip-req-build-64nk7ilr\n  Resolved https://github.com/QuentinAmbard/mandrova to commit 553986e2ab1e5e349e095b83f7c1a1e1226f99e0\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.26.4)\nCollecting sympy (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1))\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.5.3)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (3.8.4)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (24.1)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2.2.0)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1))\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.16.0)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.4/6.3 MB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.5/6.3 MB\u001B[0m \u001B[31m51.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m75.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m59.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/536.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m59.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hBuilding wheels for collected packages: mandrova\n  Building wheel for mandrova (setup.py): started\n  Building wheel for mandrova (setup.py): finished with status 'done'\n  Created wheel for mandrova: filename=mandrova-0.1-py3-none-any.whl size=23651 sha256=03b184898597b82d6adbd290d24641bec417a44b75154f0af5a3c5955eda5bb4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-pngq53nw/wheels/5f/10/00/f1f85ee04d0a466b4bdf0a5c6ac04c3a20f04d48d2b2bd9fe7\nSuccessfully built mandrova\nInstalling collected packages: mpmath, sympy, mandrova\nSuccessfully installed mandrova-0.1 mpmath-1.3.0 sympy-1.14.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nProcessing ./line_data_generator\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nBuilding wheels for collected packages: line_data_generator\n  Building wheel for line_data_generator (setup.py): started\n  Building wheel for line_data_generator (setup.py): finished with status 'done'\n  Created wheel for line_data_generator: filename=line_data_generator-0.1.0-py3-none-any.whl size=1043 sha256=e8ba3ca7296a2c54dd1e942cb13bc5189752c5166e3f950876f0e8c8d98c7b28\n  Stored in directory: /tmp/pip-ephem-wheel-cache-05y3fmdv/wheels/cf/94/8f/c5faff2777857a072e0cc5f957994c85f65c9ff547565269a7\nSuccessfully built line_data_generator\nInstalling collected packages: line_data_generator\nSuccessfully installed line_data_generator-0.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "## Parameters\n",
       "This notebook contains the parameters needed to customise the solution accelerator to you environment. Be sure to modify them before starting to ensure that the accelerator deploys correctly."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install custom data generator library\n",
    "%pip install -r ./line_data_generator/requirements.txt\n",
    "%pip install ./line_data_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4f7eb73-e69c-408d-b937-cef5b1de3063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Create Delta Table in Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a753edd3-cddd-496e-9269-1962ad5cb3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Provide required information in the 0-Parameters notebook \n",
    "\n",
    "- Your Unity Catalog table name\n",
    "  - Identify the target table you want to ingest data to. \n",
    "  \n",
    "  Do not use [Default Storage](https://docs.databricks.com/aws/en/storage/default-storage) UC tables. This is not currently supported in Zerobus Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ace668d-b1e6-4b7d-b513-7659a5d4d533",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import common paramters"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./0-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411534f0-96a9-44fa-b20a-30872a24abed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {BRONZE_TABLE} (\n",
    "    sensor_rotation DOUBLE,\n",
    "    sensor_flow DOUBLE,\n",
    "    sensor_temperature DOUBLE,\n",
    "    sensor_speed DOUBLE,\n",
    "    sensor_vibration DOUBLE,\n",
    "    sensor_pressure DOUBLE,\n",
    "    component_yield_output DOUBLE,\n",
    "    timestamp STRING,\n",
    "    component_id STRING,\n",
    "    damaged_component BOOLEAN,\n",
    "    abnormal_sensor STRING,\n",
    "    machine_id STRING,\n",
    "    line_id STRING\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f3798a-1518-46c1-bb6c-a79022617946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Generate sensor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88587f52-be0b-41ea-bfa8-cb1e4fe95691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will use a data generator to simulate data coming from a complex ball bearing production system organized as folllwing. \n",
    "\n",
    "The core elements of this prduction system are: \n",
    "- Production Line\n",
    "  - Machine\n",
    "    - Component\n",
    "      - Sensor\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/ball-bearing-diagram.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "**How does the data generator work**\n",
    "<br>\n",
    "The data genetor will produce data mathing the production line setup you defined in the Digital Twin frontend. The following variables will be used:\n",
    "\n",
    "- Number of lines\n",
    "- Number of machines per line: each line can have a different number of machies.\n",
    "- Number of components per machine: each machine  has the same number of components\n",
    "- Each component has 6 different sensors (fixed) generating data:\n",
    "  - Temperature\n",
    "  - Pressure\n",
    "  - Vibration\n",
    "  - Speed\n",
    "  - Rotation\n",
    "  - Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b30fc6-625f-4443-87ec-922e2d48b38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from line_data_generator import generate_all_lines, generate_equipment_mapping, table_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9276ecf-02ca-432a-905d-8941a0b2de5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Production Line configuration -- TODO: Read from config file\n",
    "num_lines = 4\n",
    "machines_per_line = [3, 3, 4, 2]  # Number of machines per line\n",
    "num_components = 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "268f9010-0919-4a03-85d1-065376c89689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>You can adjust the size of the generated dataset by setting the sample_size parameter.\n",
    "\n",
    "For instance, if you choose a sample size of 1000, the data generator will produce a dataset with these characteristics:\n",
    "\n",
    "- Each row represents a specific component at a specific timestamp, with component_id and timestamp serving as unique identifiers.\n",
    "- Each component will have 1000 rows, corresponding to the sample size.\n",
    "- Consecutive rows for a given component are spaced 1 millisecond apart. Thus, for a sample size of 1000, the total duration covered per component is 1 second (1000 * 0.001).\n",
    "- The total number of rows in the dataset depends on the number of components in your production line. For example, with 36 components (4 lines, 12 machines, 3 components per machine), the dataset will have 36,000 rows (36 * 1000).\n",
    "- The overall time span for the dataset remains 1 seconds (1000 * 0.001), meaning that sensor data for different components is generated in parallel at the same timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605dedb5-c642-4e0e-a348-f424f14bc22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define sample size\n",
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05ce3ba4-ddf3-45e9-9990-5d5810594a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351ead65-61b1-4166-95bb-c51e6dc25a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate equipment mapping: lines -> machine -> components -> sensors \n",
    "equipment_mapping = generate_equipment_mapping(num_lines, machines_per_line, num_components)\n",
    "\n",
    "# Estimate table size\n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows: {tot_num_rows}\")\n",
    "print(f\"Estimated table size: {est_table_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a294d0-49c1-414b-9257-a296922cf318",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756298993340}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run data generator and display the first 10 rows\n",
    "batch_df_lines = generate_all_lines(equipment_mapping, sample_size, time.time())\n",
    "display(batch_df_lines.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2922b67e-fe30-4d97-a8a5-36cd1ea9a5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Write generated data into delta table in UC\n",
    "spark.createDataFrame(batch_df_lines).write.mode(\"append\").saveAsTable(BRONZE_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afd291fa-b719-44ef-823e-3167332e8111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(BRONZE_TABLE).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8781c7f2-f259-47ba-83e5-9b1a9350fad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--\n",
    "### _STOP_ - If you want to use Zerobus Ingest to ingest data, go directly to notebook \"2-Ingest-Data-Zerobus\"\n",
    "--\n",
    "### Optional: Simulate Continuous Data Ingestion\n",
    "\n",
    "In this section, we will simulate a continuous data ingestion. To prevent creating a large dataset in a single data generation job, we will run multiple data ingestion batches. Each batch will be generated separately and appended to the Delta Table one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "428882f8-a3ad-4307-b8dc-c6617a698954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define batch configuration\n",
    "\n",
    "sample_size = 10000    # for each sensor\n",
    "batch_count = 5       # number of writes (int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45546226-ef38-4c63-a6fa-5951170273c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88283e6f-d66a-419b-81cd-9a6399867d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total number of rows to be generated \n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows in each batch: {tot_num_rows}\")\n",
    "print(f\"Estimated table size in each batch: {est_table_size:.2f} MB\")\n",
    "print(f\"Number of rows for the total dataset: {tot_num_rows * batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76f68d4e-77bc-4ef0-9bcc-c507285ac145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the data generator loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e565d5-a36f-4599-8005-b310bd912a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run loop to generate data in batch (loop) \n",
    "\n",
    "min_batch_wait =  sample_size *  0.001   # minumun seconds wait between writes to avoid having overlapping time between batches\n",
    "\n",
    "for i in range(0, int(batch_count)):\n",
    "\n",
    "  current_time = time.time()\n",
    "\n",
    "  if i > 0:\n",
    "    if current_time <= batch_time + min_batch_wait:\n",
    "      wait = int(batch_time + min_batch_wait - current_time + 10) # add 10 seconds just in case\n",
    "      time.sleep(wait)\n",
    "      print(f\"Pausing {wait} seconds to avoid overlapping timestamps across batches\")\n",
    "\n",
    "  print(f\"--- Generating batch {int(i+1)} / {int(batch_count)} ---\")\n",
    "\n",
    "  batch_time = time.time()\n",
    "  batch = generate_all_lines(equipment_mapping, sample_size, batch_time)\n",
    "\n",
    "  spark.createDataFrame(batch).write.mode(\"append\").saveAsTable(BRONZE_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c194ecf1-614f-4fa3-a8ca-c0d09b173e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(BRONZE_TABLE).count())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1-Create-Sensor-Bronze-Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}