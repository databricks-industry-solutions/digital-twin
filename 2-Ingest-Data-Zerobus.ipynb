{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff885e9-e533-4a48-a060-6ed5cce2a21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest Data with Zerobus Ingest\n",
    "\n",
    "To run this notebook you need the following:\n",
    "- Serverless Compute or Classic Compute Cluster with DBR >= 16.4 LTS\n",
    "- Zerobus Ingest Public Preview enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be591a82-963d-4a44-b865-b0b9428e4b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Zerobus Ingest\n",
    "Zerobus Ingest allows to efficiently push data into tables with ease. It supports record-by-record ingestion at any scale and operates in a serverless environment.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/zerobus-architecture.png\" width=\"600\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "- When data is transmitted to the Row Ingestion API, it goes through a buffering process before being added to a Delta table. This creates an efficient and durable ingestion mechanism to support a high volume of clients with variable throughput.\n",
    "<br>\n",
    "- Once the data has been materialized into Delta format, it becomes fully compatible with the comprehensive Databricks Platform, allowing users to leverage familiar tools and functionalities for further data analysis and processing.\n",
    "- By default, the Python SDK performs automatic recovery. When a stream fails (e.g., due to a transient network issue), the SDK will attempt to reconnect and re-ingest any unacknowledged records in the background, preserving their order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4bf8d17-6b25-4ea7-b2e9-df5608681200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f40fc77-50eb-454a-b845-daaa9d2fdad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install Zerobus Ingest SDK\n",
    "%pip install databricks-zerobus-ingest-sdk grpcio-tools\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28e53e0-e3d3-4050-8bb2-741e0a062d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Install custom data generator library\n",
    "%pip install -r ./line_data_generator/requirements.txt ./line_data_generator\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48970c5c-b6b2-4e29-8383-4c7ac0452adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Provide required information to connect to Zerobus host\n",
    "\n",
    "To use the Zerobus Ingest SDK you will need the following information:\n",
    "\n",
    "- Databricks Workspace URL \n",
    "  - Get your workspace URL. When viewing your Databricks workspace after logging in, take a look at the URL in your browser with the following format: https://_your_databricks-instance_.com/o=XXXXX. The URL that you require is everything before the “/o=XXXXX”\n",
    "\n",
    "- Your Bronze table definition\n",
    "  - Identify the target table you want to ingest data to. This is the table created in the notebook \"1. Create-Sensor-Bronze-Table\"\n",
    "\n",
    "- Zerobus Host \n",
    "  - Zerobus URI = _workspace_id_.ingest.cloud.databricks.com. Databricks will provide you with this URL\n",
    "\n",
    "- Service Principal Id and Secret --->  _For the following steps you need to be a workspace admin. If you are not an admin, ask to the appropriate person in your organization_\n",
    "  - Go to Settings > Identity and Access.\n",
    "  - Generate and save client ID and secret for that Service Principal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8d6a4ae-5ec2-4b83-83f2-db44da93b8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Once you have this information, update the 0-Parameters.ipynb notebook with your own values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f3585d-5d80-44cf-a74d-ec821289d204",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import common parameters"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./0-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a2f9a51-2528-4edb-abe3-60bb916acc17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Generate Proto file\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff90dc7-d7c2-4f16-a906-10de79b8e6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "proto_msg = \"DigitalTwin\"       # Do not edit\n",
    "proto_name = \"dt-solacc.proto\"  # Do not edit\n",
    "\n",
    "\n",
    "command = [\n",
    "  \"python\", \"-m\", \"zerobus.tools.generate_proto\",\n",
    "    \"--uc-endpoint\", WORKSPACE_URL, \n",
    "    \"--client-id\", CLIENT_ID,\n",
    "    \"--client-secret\", CLIENT_SECRET,\n",
    "    \"--table\", BRONZE_TABLE,\n",
    "    \"--proto-msg\", proto_msg,\n",
    "    \"--output\", proto_name,\n",
    "]\n",
    "\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "command = [\n",
    "    \"python3\", \"-m\", \"grpc_tools.protoc\", \"-I.\", \"--python_out=.\",\n",
    "    proto_name\n",
    "]\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "print(f\"Proto compiled: {proto_name.replace('.proto', '_pb2.py')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6466eb5b-27cd-421a-b36d-aed6bdb1f256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart to avoid issues to load the proto definition\n",
    "%restart_python\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af13f37b-b801-4b0f-9a17-1af910dcdd4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import common parameters"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./0-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95530022-d9f2-495b-96ea-a94600eb6767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Setup connection to the output Delta Table via Zerobus Ingest\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd68cf68-b145-4d52-8fff-14fd1651c2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we need to grant access to the bronze table created in Notebook 1 to the Service Principal you will use to connect to Zerobus Ingest Host.\n",
    "\n",
    "A Service Principal is a specialized identity providing more security than personalized accounts. More information regarding Service Principal and how to use them for authentication can be found in [these instructions](https://docs.databricks.com/aws/en/dev-tools/auth/oauth-m2m).\n",
    "\n",
    "The Service Principal needs to be granted the folllwing permission on the destination table:\n",
    "  - For the catalog: USE_CATALOG.\n",
    "  - For the schema: USE_SCHEMA.\n",
    "  - For the table: MODIFY, SELECT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a757e2ec-c522-4e27-bfce-a1d4aa22842f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Grant MODIFY and SELECT on the table to the Service Principal\n",
    "spark.sql(f\"GRANT MODIFY, SELECT ON TABLE {BRONZE_TABLE} TO `{CLIENT_ID}`\")\n",
    "\n",
    "# Extract catalog and schema from the table name\n",
    "catalog, schema, _ = BRONZE_TABLE.split(\".\")\n",
    "\n",
    "# Grant USE CATALOG on the catalog to the Service Principal\n",
    "spark.sql(f\"GRANT USE CATALOG ON CATALOG {catalog} TO `{CLIENT_ID}`\")\n",
    "\n",
    "# Grant USE SCHEMA on the schema to the Service Principal\n",
    "spark.sql(f\"GRANT USE SCHEMA ON SCHEMA {catalog}.{schema} TO `{CLIENT_ID}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbfc15e-b429-4feb-8438-73298396b8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, we can start using the Zerobus SDK to connect to Zerobus Ingest Host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b17767f-20ac-419b-bb7f-740afcbf2bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dt_solacc_pb2 as row_pb2  # This is the compiled proto definition\n",
    "from zerobus.sdk.aio import ZerobusSdk  # asynchronous SDK\n",
    "from zerobus.sdk.shared import TableProperties  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412e3565-da3b-4d9b-96c1-ede3f2f898b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define connection to Zerobus Ingest Host \n",
    "\n",
    "table_properties = TableProperties(BRONZE_TABLE, row_pb2.DigitalTwin.DESCRIPTOR)\n",
    "sdk = ZerobusSdk(ZEROBUS_URL, WORKSPACE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756e01a6-5045-44a6-93e8-0277a696ed39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test connection \n",
    " \n",
    "stream = await sdk.create_stream(CLIENT_ID, CLIENT_SECRET, table_properties)\n",
    "\n",
    "if stream:\n",
    "    print(\"Connection successful\")\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217e1cb6-4d15-4c88-bb63-e1f5aa7ace0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Generate sensor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3049523-09c0-43e1-8377-4b0618e66043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will use a data generator to simulate data coming from a complex ball bearing production system organized as folllwing. \n",
    "\n",
    "The core elements of this prduction system are: \n",
    "- Production Line\n",
    "  - Machine\n",
    "    - Component\n",
    "      - Sensor\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/ball-bearing-diagram.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "**How does the data generator work**\n",
    "<br>\n",
    "The data genetor will produce data mathing the production line setup you defined in the Digital Twin frontend. The following variables will be used:\n",
    "\n",
    "- Number of lines\n",
    "- Number of machines per line: each line can have a different number of machies.\n",
    "- Number of components per machine: each machine  has the same number of components\n",
    "- Each component has 6 different sensors (fixed) generating data:\n",
    "  - Temperature\n",
    "  - Pressure\n",
    "  - Vibration\n",
    "  - Speed\n",
    "  - Rotation\n",
    "  - Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa45ffe-725e-4c08-aa1a-a9903a397283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from line_data_generator import generate_all_lines, generate_equipment_mapping, table_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c443948-3ae9-4e8a-9924-98708d34866c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Production Line configuration -- TODO: Read from config file\n",
    "num_lines = 4\n",
    "machines_per_line = [3, 3, 4, 2]  # Number of machines per line\n",
    "num_components = 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb8c9e6-d5b0-4498-9131-c569fed54b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>You can adjust the size of the generated dataset by setting the sample_size parameter.\n",
    "\n",
    "For instance, if you choose a sample size of 1000, the data generator will produce a dataset with these characteristics:\n",
    "\n",
    "- Each row represents a specific component at a specific timestamp, with component_id and timestamp serving as unique identifiers.\n",
    "- Each component will have 1000 rows, corresponding to the sample size.\n",
    "- Consecutive rows for a given component are spaced 1 millisecond apart. Thus, for a sample size of 1000, the total duration covered per component is 1 second (1000 * 0.001).\n",
    "- The total number of rows in the dataset depends on the number of components in your production line. For example, with 36 components (4 lines, 12 machines, 3 components per machine), the dataset will have 36,000 rows (36 * 1000).\n",
    "- The overall time span for the dataset remains 1 seconds (1000 * 0.001), meaning that sensor data for different components is generated in parallel at the same timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d60ad8-5360-4ec2-96b0-5ef4951ce3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define sample size\n",
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a029e3a8-abd0-4bf3-89d8-154156accbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc643337-0de3-4ecb-b0bb-b0b1bdc3ce17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate equipment mapping: lines -> machine -> components -> sensors \n",
    "equipment_mapping = generate_equipment_mapping(num_lines, machines_per_line, num_components)\n",
    "\n",
    "# Estimate table size\n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows: {tot_num_rows}\")\n",
    "print(f\"Estimated table size: {est_table_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170ae4a2-99d6-4f4e-b723-b6928a3682ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run data generator and display the first 10 rows\n",
    "batch_df_lines = generate_all_lines(equipment_mapping, sample_size, time.time())\n",
    "display(batch_df_lines.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2964ef-2c9b-45aa-b576-7afe7abd01ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Ingest data via Zerobus API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95893b07-c5f0-4016-81dc-590ffbcd26e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Zerobus Python SDK is available in two variations:\n",
    "\n",
    "- **Synchronous (sync)**: A traditional, blocking client that uses standard threading for concurrency. This is the default client.\n",
    "- **Asynchronous (async)**: A non-blocking client based on Python's asyncio library, suitable for high-throughput I/O-bound applications.\n",
    "\n",
    "In this notebook we will use Asynchronous non-blocking clients. With the asynchronous client, _ingest_record_ is an async method that returns an asyncio.Future. You can await this _future_ to block until the record is durable.\n",
    "\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3179fae-a4f1-4aeb-a154-690de83e6b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8401fa-4fd2-4629-9ee0-76f625a0c3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "A data producer will first open a “stream” to a Delta table, construct a message matching its schema, and then push the message to our API. Zerobus will make the data durable, acknowledge the client's message, and materialize in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d7daa2-e0f7-4fd4-822a-14aaf5199564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def asynchronous_non_blocking_call():\n",
    "\n",
    "    # Create stream to table\n",
    "    stream = await sdk.create_stream(CLIENT_ID, CLIENT_SECRET, table_properties)\n",
    "\n",
    "    # Config progress bar\n",
    "    total = len(batch_df_lines)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=\"Ingesting sensor data via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    # non-blocking (streaming) call to Zerobus \n",
    "    for i in range(total):\n",
    "        \n",
    "        row = batch_df_lines.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "\n",
    "    await stream.close()\n",
    "\n",
    "    # Add ingest info\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Rows ingested: {total}\")\n",
    "    print(f\"Time elapsed: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eafe1a7-c664-462f-95d8-9d7452520496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the client\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await asynchronous_non_blocking_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2deca82a-38a4-4191-8ee4-551d6b506743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display destination table\n",
    "display(spark.table(BRONZE_TABLE).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493d952b-70f5-4649-ba37-1bb35b0aa4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Simulate data ingestion from  multiple sources in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033accb8-546b-4716-bee1-8a3e56560dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section we will simulate data ingestion from multiple production lines in parallel. Zerobus Ingest will process each production line separately and push the data in the same Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3024109d-39bb-48ad-803a-d0b85d558a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from line_data_generator import generate_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb3cbbf-305d-46a1-923a-a93ddd262617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define sample size for each production line batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf79e58-95b3-4160-8281-ae6f193b71f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample size for a component in each line\n",
    "line_sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa6ec8f-a5e4-4ca2-a3c3-638f00ea226f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6df702-3ea2-49e3-86d2-588df4144dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, line_sample_size)\n",
    "\n",
    "for line in range(len(line_num_rows)): \n",
    "  print(f\"Line {line+1}\")               \n",
    "  print(f\" - Number of rows: {line_num_rows[line]}\")\n",
    "  print(f\" - Estimated table size: {est_line_table_size[line]:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227c4518-935b-4926-b034-d6078837b381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "The strategy to ingest data from multiple sources in parallel depends on whether you are using the sync or async client. <br> The asynchronous SDK uses an event loop on a single thread. To achieve parallelism, you create multiple asyncio tasks and run them concurrently using asyncio.gather. This is highly efficient for I/O-bound operations.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f50d2ab-5179-44f0-92c1-dc579ab7f212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def worker(worker_id: int):\n",
    "    \"\"\"An async worker that creates a stream and ingests records.\"\"\"\n",
    "    print(f\"Worker {worker_id}: Starting...\")\n",
    "\n",
    "    stream = await sdk.create_stream(CLIENT_ID, CLIENT_SECRET, table_properties)\n",
    "\n",
    "    line_number = worker_id\n",
    "    current_time = time.time()\n",
    "    batch_df_line = generate_line(line_number, equipment_mapping, line_sample_size, current_time)\n",
    "\n",
    "    total = len(batch_df_line)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=f\"Ingesting sensor data for Line {line_number+1} via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(total):\n",
    "\n",
    "        row = batch_df_line.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data for Line {line_number+1} via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "    await stream.close()\n",
    "    print(f\"Worker {worker_id}: Finished.\")\n",
    "\n",
    "\n",
    "async def run_parallel():\n",
    "    num_workers = num_lines\n",
    "    tasks = []\n",
    "    for i in range(num_workers):\n",
    "        task = asyncio.create_task(worker(i))\n",
    "        tasks.append(task)\n",
    "       \n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07785b57-1e2b-49dc-866f-523bf31eeb01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the client\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await run_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ee467c-9bb6-4d39-8429-7205bae28236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Simulate Continuous Data Ingestion from  one source\n",
    "\n",
    "In this section, we will simulate a continuous data ingestion. To prevent creating a large dataset in a single data generation job, we will run multiple data ingestion batches. Each batch will be generated separately and sent to the Delta Table one at a time using Zerobus Ingest. <br>\n",
    "<br>\n",
    "You can configuare the batch sample size and batch count to simulate the desired contunous data ingestion duration.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24ff529-778e-4703-963f-87525722b40a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define batch configuration\n",
    "sample_size = 10000   # for each batch\n",
    "batch_count = 10   # number of batches to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a357ede-7627-49ad-885e-6c0f144d14c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af96d931-0c24-4950-a485-a87c415ffaa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total number of rows to be generated \n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows in each batch: {tot_num_rows}\")\n",
    "print(f\"Estimated table size in each batch: {est_table_size:.2f} MB\")\n",
    "print(f\"Number of rows for the total dataset: {tot_num_rows * batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1924a473-83bd-4d2b-898c-e1667c36fd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "We will use the same clinet configuration used in Step 4.\n",
    "For each batch,  a data producer will first open a “stream” to a Delta table, construct a message matching its schema, and then push the message to our API. Zerobus will make the data durable, acknowledge the client's message, and materialize in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6994e2-c66e-4288-9631-42240e7e9978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def batch_asynchronous_blocking_call():\n",
    "\n",
    "  # Create stream to table\n",
    "  stream = await sdk.create_stream(CLIENT_ID, CLIENT_SECRET, table_properties)\n",
    "\n",
    "  min_batch_wait =  sample_size *  0.001   # minumun seconds wait between writes to avoid having overlapping time between batches\n",
    "\n",
    "  for i in range(0, int(batch_count)):\n",
    "    \n",
    "    current_time = time.time()\n",
    "\n",
    "    if i > 0:\n",
    "      if current_time <= batch_time + min_batch_wait:\n",
    "        wait = int(batch_time + min_batch_wait - current_time + 10) # add 10 seconds just in case\n",
    "        time.sleep(wait)\n",
    "        print(f\"Pausing {wait} seconds to avoid overlapping timestamps across batches\")\n",
    "\n",
    "    print(f\"--- Ingesting batch {int(i+1)} / {int(batch_count)} ---\")\n",
    "\n",
    "    # Generate new data\n",
    "    batch_time = time.time()\n",
    "    batch_df_lines = generate_all_lines(equipment_mapping, sample_size, batch_time)\n",
    "\n",
    "    # Config progress bar\n",
    "    total = len(batch_df_lines)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=\"Ingesting sensor data via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    # non-blocking (streaming) call. \n",
    "    for i in range(total):\n",
    "        \n",
    "        row = batch_df_lines.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "\n",
    "    # Add ingest info\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Rows ingested: {total}\")\n",
    "    print(f\"Time elapsed: {elapsed:.2f} seconds\")\n",
    "    print(\"\\n\" * 1)\n",
    "\n",
    "  await stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the data generator simulation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_simulation = False  # change to True if you want to run the simulation job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13c6241-2a14-4398-bac1-5680e6891928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if run_simulation:\n",
    "\n",
    "    # Run the asynchronous function\n",
    "    import nest_asyncio\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "    await batch_asynchronous_blocking_call()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2-Ingest-Data-Zerobus",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
