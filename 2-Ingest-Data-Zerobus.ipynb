{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dff885e9-e533-4a48-a060-6ed5cce2a21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest Data with Zerobus Ingest\n",
    "\n",
    "To run this notebook you need the following:\n",
    "- Classic Compute Cluster - DBR >= 16.4 LTS. Serverless compute is currently not supported\n",
    "- Zerobus Ingest Public Preview enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be591a82-963d-4a44-b865-b0b9428e4b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Zerobus Ingest\n",
    "Zerobus Ingest allows to efficiently push data into tables with ease. It supports record-by-record ingestion at any scale and operates in a serverless environment.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./images/zerobus-architecture.png\" width=\"600\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "- When data is transmitted to the Row Ingestion API, it goes through a buffering process before being added to a Delta table. This creates an efficient and durable ingestion mechanism to support a high volume of clients with variable throughput.\n",
    "<br>\n",
    "- Once the data has been materialized into Delta format, it becomes fully compatible with the comprehensive Databricks Platform, allowing users to leverage familiar tools and functionalities for further data analysis and processing.\n",
    "- By default, the Python SDK performs automatic recovery. When a stream fails (e.g., due to a transient network issue), the SDK will attempt to reconnect and re-ingest any unacknowledged records in the background, preserving their order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4bf8d17-6b25-4ea7-b2e9-df5608681200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f40fc77-50eb-454a-b845-daaa9d2fdad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./databricks_zerobus-0.0.17-py3-none-any.whl\nCollecting grpcio-tools\n  Downloading grpcio_tools-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.3 kB)\nRequirement already satisfied: protobuf in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-zerobus==0.0.17) (5.29.5)\nRequirement already satisfied: grpcio in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-zerobus==0.0.17) (1.75.1)\nRequirement already satisfied: requests in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from databricks-zerobus==0.0.17) (2.32.5)\nCollecting protobuf (from databricks-zerobus==0.0.17)\n  Downloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from grpcio-tools) (74.0.0)\nRequirement already satisfied: typing-extensions~=4.12 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from grpcio->databricks-zerobus==0.0.17) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->databricks-zerobus==0.0.17) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->databricks-zerobus==0.0.17) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (from requests->databricks-zerobus==0.0.17) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->databricks-zerobus==0.0.17) (2024.6.2)\nDownloading grpcio_tools-1.75.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.7 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.7 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.2/2.7 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/2.7 MB\u001B[0m \u001B[31m25.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.7/2.7 MB\u001B[0m \u001B[31m30.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/322.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.0/322.0 kB\u001B[0m \u001B[31m36.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: protobuf, grpcio-tools, databricks-zerobus\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 5.29.5\n    Not uninstalling protobuf at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndbt-adapters 1.9.0 requires protobuf<5.0,>=3.0, but you have protobuf 6.32.1 which is incompatible.\ndbt-common 1.12.0 requires protobuf<5.0.0,>=4.0.0, but you have protobuf 6.32.1 which is incompatible.\ndbt-core 1.8.7 requires protobuf<5,>=4.0.0, but you have protobuf 6.32.1 which is incompatible.\ndbt-databricks 1.8.7 requires databricks-sdk==0.17.0, but you have databricks-sdk 0.67.0 which is incompatible.\ndbt-databricks 1.8.7 requires protobuf<5.0.0, but you have protobuf 6.32.1 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\ngoogle-api-core 2.20.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\ngoogleapis-common-protos 1.65.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\nproto-plus 1.24.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 6.32.1 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed databricks-zerobus-0.0.17 grpcio-tools-1.75.1 protobuf-6.32.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Zerobus Ingest SDK -- CURRENTLY REQUIRES PRIVATE PREVIEW SDK\n",
    "%pip install databricks_zerobus-0.0.17-py3-none-any.whl grpcio-tools\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b28e53e0-e3d3-4050-8bb2-741e0a062d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./line_data_generator\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting git+https://github.com/QuentinAmbard/mandrova (from -r ./line_data_generator/requirements.txt (line 1))\n  Cloning https://github.com/QuentinAmbard/mandrova to /tmp/pip-req-build-giuga211\n  Running command git clone --filter=blob:none --quiet https://github.com/QuentinAmbard/mandrova /tmp/pip-req-build-giuga211\n  Resolved https://github.com/QuentinAmbard/mandrova to commit 553986e2ab1e5e349e095b83f7c1a1e1226f99e0\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.26.4)\nCollecting sympy (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1))\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.5.3)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.12/site-packages (from mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (3.8.4)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (4.51.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (24.1)\nRequirement already satisfied: pillow>=8 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.12/site-packages (from matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2024.1)\nRequirement already satisfied: scipy>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from scikit-learn->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (2.2.0)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1))\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->mandrova==0.1->-r ./line_data_generator/requirements.txt (line 1)) (1.16.0)\nDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/6.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.9/6.3 MB\u001B[0m \u001B[31m27.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m112.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m86.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/536.2 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m83.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hBuilding wheels for collected packages: line_data_generator, mandrova\n  Building wheel for line_data_generator (setup.py): started\n  Building wheel for line_data_generator (setup.py): finished with status 'done'\n  Created wheel for line_data_generator: filename=line_data_generator-0.1.0-py3-none-any.whl size=1043 sha256=232b3f5d21dfa477df98528dacdf9231d80d51c86f5b5884359ffaa0e1427f86\n  Stored in directory: /tmp/pip-ephem-wheel-cache-48j3b0gp/wheels/cf/94/8f/c5faff2777857a072e0cc5f957994c85f65c9ff547565269a7\n  Building wheel for mandrova (setup.py): started\n  Building wheel for mandrova (setup.py): finished with status 'done'\n  Created wheel for mandrova: filename=mandrova-0.1-py3-none-any.whl size=23651 sha256=3d1c11c1549b9838737ba57f586ae0ae8f45daf30a33c1ed2482e3fbdd2d6a00\n  Stored in directory: /tmp/pip-ephem-wheel-cache-48j3b0gp/wheels/5f/10/00/f1f85ee04d0a466b4bdf0a5c6ac04c3a20f04d48d2b2bd9fe7\nSuccessfully built line_data_generator mandrova\nInstalling collected packages: mpmath, line_data_generator, sympy, mandrova\nSuccessfully installed line_data_generator-0.1.0 mandrova-0.1 mpmath-1.3.0 sympy-1.14.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install custom data generator library\n",
    "%pip install -r ./line_data_generator/requirements.txt ./line_data_generator\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48970c5c-b6b2-4e29-8383-4c7ac0452adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Provide required information to connect to Zerobus host\n",
    "\n",
    "To use the Zerobus Ingest SDK you will need the following information:\n",
    "\n",
    "- Databricks Workspace URL \n",
    "  - Get your workspace URL. When viewing your Databricks workspace after logging in, take a look at the URL in your browser with the following format: https://_your_databricks-instance_.com/o=XXXXX. The URL that you require is everything before the “/o=XXXXX”\n",
    "\n",
    "- Your table name\n",
    "  - Identify the target table you want to ingest data to. This is the table created in the notebook \"1. Create-Sensor-Bronze-Table\"\n",
    "\n",
    "- Token\n",
    "  - A Personal Access Token (PAT) is required to authenticate and identify who the client is and whether they have access to the target table. Follow [these instructions](https://docs.databricks.com/en/dev-tools/auth/pat.html#databricks-personal-access-tokens-for-workspace-users) to create a PAT.    \n",
    "\n",
    "- Zerobus Host \n",
    "  - Zerobus URI = _workspace_id_.ingest.cloud.databricks.com. Databricks will provide you with this URL\n",
    "\n",
    "- Service Principal --->  _For the following steps you need to be a workspace admin. If you are not an admin, ask to the appropriate person in your organization_\n",
    "  - Go to Settings > Identity and Access.\n",
    "  - Generate and save client ID and secret for that Service Principal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8d6a4ae-5ec2-4b83-83f2-db44da93b8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Once you have this information, update the 0-Parameters.ipynb notebook with your own values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f3585d-5d80-44cf-a74d-ec821289d204",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import common parameters"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "## Parameters\n",
       "This notebook contains the parameters needed to customise the solution accelerator to you environment. Be sure to modify them before starting to ensure that the accelerator deploys correctly."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./0-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a2f9a51-2528-4edb-abe3-60bb916acc17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Generate Proto file\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff90dc7-d7c2-4f16-a906-10de79b8e6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated proto file at: dt-solacc.proto\nProto compiled: dt-solacc_pb2.py\n"
     ]
    }
   ],
   "source": [
    "# Generate and compile proto file for Zerobus ingestion\n",
    "import subprocess\n",
    "\n",
    "\n",
    "proto_msg = \"DigitalTwin\"       # Do not edit\n",
    "proto_name = \"dt-solacc.proto\"  # Do not edit\n",
    "\n",
    "command = [\n",
    "    \"generate_proto\",\n",
    "    \"--uc-endpoint\", WORKSPACE_URL,\n",
    "    \"--uc-token\", PAT,\n",
    "    \"--table\", BRONZE_TABLE,\n",
    "    \"--proto-msg\", proto_msg, \n",
    "    \"--output\", proto_name\n",
    "]\n",
    "subprocess.run(command, check=True)\n",
    "\n",
    "command = [\n",
    "    \"python3\", \"-m\", \"grpc_tools.protoc\", \"-I.\", \"--python_out=.\",\n",
    "    proto_name\n",
    "]\n",
    "subprocess.run(command, check=True)\n",
    "print(f\"Proto compiled: {proto_name.replace('.proto', '_pb2.py')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6466eb5b-27cd-421a-b36d-aed6bdb1f256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Restart to avoid issues to load the proto definition\n",
    "%restart_python\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af13f37b-b801-4b0f-9a17-1af910dcdd4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import common parameters"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "## Parameters\n",
       "This notebook contains the parameters needed to customise the solution accelerator to you environment. Be sure to modify them before starting to ensure that the accelerator deploys correctly."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ./0-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95530022-d9f2-495b-96ea-a94600eb6767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Setup connection to the output Delta Table via Zerobus Ingest\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd68cf68-b145-4d52-8fff-14fd1651c2de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we need to grant access to the bronze table created in Notebook 1 to the Service Principal you will use to connect to Zerobus Ingest Host.\n",
    "\n",
    "A Service Principal is a specialized identity providing more security than personalized accounts. More information regarding Service Principal and how to use them for authentication can be found in [these instructions](https://docs.databricks.com/aws/en/dev-tools/auth/oauth-m2m).\n",
    "\n",
    "The Service Principal needs to be granted the folllwing permission on the destination table:\n",
    "  - For the catalog: USE_CATALOG.\n",
    "  - For the schema: USE_SCHEMA.\n",
    "  - For the table: MODIFY, SELECT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a757e2ec-c522-4e27-bfce-a1d4aa22842f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grant MODIFY and SELECT on the table to the Service Principal\n",
    "spark.sql(f\"GRANT MODIFY, SELECT ON TABLE {BRONZE_TABLE} TO `{CLIENT_ID}`\")\n",
    "\n",
    "# Extract catalog and schema from the table name\n",
    "catalog, schema, _ = BRONZE_TABLE.split(\".\")\n",
    "\n",
    "# Grant USE CATALOG on the catalog to the Service Principal\n",
    "spark.sql(f\"GRANT USE CATALOG ON CATALOG {catalog} TO `{CLIENT_ID}`\")\n",
    "\n",
    "# Grant USE SCHEMA on the schema to the Service Principal\n",
    "spark.sql(f\"GRANT USE SCHEMA ON SCHEMA {catalog}.{schema} TO `{CLIENT_ID}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbfc15e-b429-4feb-8438-73298396b8eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, we can start using the Zerobus SDK to connect to Zerobus Ingest Host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b17767f-20ac-419b-bb7f-740afcbf2bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dt_solacc_pb2 as row_pb2  # This is the compiled proto definition\n",
    "from zerobus_sdk.aio import ZerobusSdk  # asynchronous SDK\n",
    "from zerobus_sdk import TableProperties, StreamConfigurationOptions, get_zerobus_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412e3565-da3b-4d9b-96c1-ede3f2f898b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Zerobus Ingest Host \n",
    "table_properties = TableProperties(BRONZE_TABLE, row_pb2.DigitalTwin.DESCRIPTOR)\n",
    "sdk_handle = ZerobusSdk(ZEROBUS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756e01a6-5045-44a6-93e8-0277a696ed39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAioRpcError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:128\u001B[0m, in \u001B[0;36mZerobusStream.__create_stream\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    127\u001B[0m success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[0;32m--> 128\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n",
       "\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mHasField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreate_stream_response\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/grpc/aio/_call.py:368\u001B[0m, in \u001B[0;36m_StreamResponseMixin._fetch_stream_responses\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    367\u001B[0m \u001B[38;5;66;03m# If the read operation failed, Core should explain why.\u001B[39;00m\n",
       "\u001B[0;32m--> 368\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_for_status()\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/grpc/aio/_call.py:274\u001B[0m, in \u001B[0;36mCall._raise_for_status\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m code \u001B[38;5;241m!=\u001B[39m grpc\u001B[38;5;241m.\u001B[39mStatusCode\u001B[38;5;241m.\u001B[39mOK:\n",
       "\u001B[0;32m--> 274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _create_rpc_error(\n",
       "\u001B[1;32m    275\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_metadata(),\n",
       "\u001B[1;32m    276\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cython_call\u001B[38;5;241m.\u001B[39mstatus(),\n",
       "\u001B[1;32m    277\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mAioRpcError\u001B[0m: <AioRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAUTHENTICATED\n",
       "\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mNonRetriableException\u001B[0m                     Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8287725918172050>, line 15\u001B[0m\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDATABRICKS_CLIENT_ID and DATABRICKS_CLIENT_SECRET environment variables are required for UC table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      5\u001B[0m     )\n",
       "\u001B[1;32m      7\u001B[0m options \u001B[38;5;241m=\u001B[39m StreamConfigurationOptions(\n",
       "\u001B[1;32m      8\u001B[0m     token_factory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m: get_zerobus_token(\n",
       "\u001B[1;32m      9\u001B[0m         BRONZE_TABLE, \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m         CLIENT_ID, \n",
       "\u001B[1;32m     13\u001B[0m         CLIENT_SECRET))\n",
       "\u001B[0;32m---> 15\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m sdk_handle\u001B[38;5;241m.\u001B[39mcreate_stream(table_properties, options)\n",
       "\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection successful\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:729\u001B[0m, in \u001B[0;36mZerobusSdk.create_stream\u001B[0;34m(self, table_properties, options)\u001B[0m\n",
       "\u001B[1;32m    727\u001B[0m stub \u001B[38;5;241m=\u001B[39m zerobus_service_pb2_grpc\u001B[38;5;241m.\u001B[39mZerobusStub(channel)\n",
       "\u001B[1;32m    728\u001B[0m stream \u001B[38;5;241m=\u001B[39m ZerobusStream(stub, table_properties, options)\n",
       "\u001B[0;32m--> 729\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m stream\u001B[38;5;241m.\u001B[39m_initialize()\n",
       "\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:163\u001B[0m, in \u001B[0;36mZerobusStream._initialize\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    162\u001B[0m     max_attempts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mrecovery_retries \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mrecovery \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__with_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__create_stream, max_attempts)\n",
       "\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_state(StreamState\u001B[38;5;241m.\u001B[39mOPENED)\n",
       "\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ZerobusException:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:91\u001B[0m, in \u001B[0;36mZerobusStream.__with_retries\u001B[0;34m(self, func, max_attempts)\u001B[0m\n",
       "\u001B[1;32m     89\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStream operation timed out...\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NonRetriableException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m---> 91\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mCancelledError:\n",
       "\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:83\u001B[0m, in \u001B[0;36mZerobusStream.__with_retries\u001B[0;34m(self, func, max_attempts)\u001B[0m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mwait_for(func(), timeout\u001B[38;5;241m=\u001B[39mtimeout_seconds)\n",
       "\u001B[1;32m     84\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
       "\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mTimeoutError:\n",
       "\u001B[1;32m     86\u001B[0m         \u001B[38;5;66;03m# We add from None to suppress the stack trace generated by asyncio\u001B[39;00m\n",
       "\u001B[1;32m     87\u001B[0m         \u001B[38;5;66;03m# (a lot of timeout and cancelled errors)\u001B[39;00m\n",
       "\u001B[1;32m     88\u001B[0m         \u001B[38;5;66;03m# This makes the exception trace much cleaner when shown to the user\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/usr/lib/python3.12/asyncio/tasks.py:520\u001B[0m, in \u001B[0;36mwait_for\u001B[0;34m(fut, timeout)\u001B[0m\n",
       "\u001B[1;32m    517\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
       "\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m timeouts\u001B[38;5;241m.\u001B[39mtimeout(timeout):\n",
       "\u001B[0;32m--> 520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m fut\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:150\u001B[0m, in \u001B[0;36mZerobusStream.__create_stream\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mcode() \u001B[38;5;129;01min\u001B[39;00m NOT_RETRIABLE_GRPC_CODES:\n",
       "\u001B[1;32m    148\u001B[0m     \u001B[38;5;66;03m# Non-retriable gRPC errors\u001B[39;00m\n",
       "\u001B[1;32m    149\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNon-retriable gRPC error during stream creation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NonRetriableException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to create a stream: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# Retriable gRPC errors\u001B[39;00m\n",
       "\u001B[1;32m    153\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetriable gRPC error during stream creation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNonRetriableException\u001B[0m: Failed to create a stream: <AioRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAUTHENTICATED\n",
       "\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n",
       ">"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NonRetriableException",
        "evalue": "Failed to create a stream: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAUTHENTICATED\n\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n>"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NonRetriableException</span>: Failed to create a stream: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAUTHENTICATED\n\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n>"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAioRpcError\u001B[0m                               Traceback (most recent call last)",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:128\u001B[0m, in \u001B[0;36mZerobusStream.__create_stream\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    127\u001B[0m success \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 128\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream:\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mHasField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreate_stream_response\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/grpc/aio/_call.py:368\u001B[0m, in \u001B[0;36m_StreamResponseMixin._fetch_stream_responses\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;66;03m# If the read operation failed, Core should explain why.\u001B[39;00m\n\u001B[0;32m--> 368\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_for_status()\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages/grpc/aio/_call.py:274\u001B[0m, in \u001B[0;36mCall._raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m code \u001B[38;5;241m!=\u001B[39m grpc\u001B[38;5;241m.\u001B[39mStatusCode\u001B[38;5;241m.\u001B[39mOK:\n\u001B[0;32m--> 274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _create_rpc_error(\n\u001B[1;32m    275\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitial_metadata(),\n\u001B[1;32m    276\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cython_call\u001B[38;5;241m.\u001B[39mstatus(),\n\u001B[1;32m    277\u001B[0m     )\n",
        "\u001B[0;31mAioRpcError\u001B[0m: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAUTHENTICATED\n\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001B[0;31mNonRetriableException\u001B[0m                     Traceback (most recent call last)",
        "File \u001B[0;32m<command-8287725918172050>, line 15\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDATABRICKS_CLIENT_ID and DATABRICKS_CLIENT_SECRET environment variables are required for UC table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m     )\n\u001B[1;32m      7\u001B[0m options \u001B[38;5;241m=\u001B[39m StreamConfigurationOptions(\n\u001B[1;32m      8\u001B[0m     token_factory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m: get_zerobus_token(\n\u001B[1;32m      9\u001B[0m         BRONZE_TABLE, \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m         CLIENT_ID, \n\u001B[1;32m     13\u001B[0m         CLIENT_SECRET))\n\u001B[0;32m---> 15\u001B[0m stream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m sdk_handle\u001B[38;5;241m.\u001B[39mcreate_stream(table_properties, options)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection successful\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:729\u001B[0m, in \u001B[0;36mZerobusSdk.create_stream\u001B[0;34m(self, table_properties, options)\u001B[0m\n\u001B[1;32m    727\u001B[0m stub \u001B[38;5;241m=\u001B[39m zerobus_service_pb2_grpc\u001B[38;5;241m.\u001B[39mZerobusStub(channel)\n\u001B[1;32m    728\u001B[0m stream \u001B[38;5;241m=\u001B[39m ZerobusStream(stub, table_properties, options)\n\u001B[0;32m--> 729\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m stream\u001B[38;5;241m.\u001B[39m_initialize()\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:163\u001B[0m, in \u001B[0;36mZerobusStream._initialize\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     max_attempts \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mrecovery_retries \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mrecovery \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__with_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__create_stream, max_attempts)\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_state(StreamState\u001B[38;5;241m.\u001B[39mOPENED)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ZerobusException:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:91\u001B[0m, in \u001B[0;36mZerobusStream.__with_retries\u001B[0;34m(self, func, max_attempts)\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStream operation timed out...\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NonRetriableException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m---> 91\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mCancelledError:\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:83\u001B[0m, in \u001B[0;36mZerobusStream.__with_retries\u001B[0;34m(self, func, max_attempts)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     82\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 83\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mwait_for(func(), timeout\u001B[38;5;241m=\u001B[39mtimeout_seconds)\n\u001B[1;32m     84\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mTimeoutError:\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;66;03m# We add from None to suppress the stack trace generated by asyncio\u001B[39;00m\n\u001B[1;32m     87\u001B[0m         \u001B[38;5;66;03m# (a lot of timeout and cancelled errors)\u001B[39;00m\n\u001B[1;32m     88\u001B[0m         \u001B[38;5;66;03m# This makes the exception trace much cleaner when shown to the user\u001B[39;00m\n",
        "File \u001B[0;32m/usr/lib/python3.12/asyncio/tasks.py:520\u001B[0m, in \u001B[0;36mwait_for\u001B[0;34m(fut, timeout)\u001B[0m\n\u001B[1;32m    517\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m timeouts\u001B[38;5;241m.\u001B[39mtimeout(timeout):\n\u001B[0;32m--> 520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m fut\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-5705da3f-12a5-4f7c-b85f-f5d6a5441eab/lib/python3.12/site-packages/zerobus_sdk/aio/zerobus_sdk.py:150\u001B[0m, in \u001B[0;36mZerobusStream.__create_stream\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mcode() \u001B[38;5;129;01min\u001B[39;00m NOT_RETRIABLE_GRPC_CODES:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;66;03m# Non-retriable gRPC errors\u001B[39;00m\n\u001B[1;32m    149\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNon-retriable gRPC error during stream creation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NonRetriableException(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to create a stream: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# Retriable gRPC errors\u001B[39;00m\n\u001B[1;32m    153\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetriable gRPC error during stream creation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNonRetriableException\u001B[0m: Failed to create a stream: <AioRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAUTHENTICATED\n\tdetails = \"Authorization token is not valid. Error Code: 2, Error State: 2.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:16, grpc_message:\"Authorization token is not valid. Error Code: 2, Error State: 2.\"}\"\n>"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test connection \n",
    "if not all([CLIENT_ID, CLIENT_SECRET]):\n",
    "    raise ValueError(\n",
    "        \"DATABRICKS_CLIENT_ID and DATABRICKS_CLIENT_SECRET environment variables are required for UC table\"\n",
    "    )\n",
    "\n",
    "options = StreamConfigurationOptions(\n",
    "    token_factory=lambda: get_zerobus_token(\n",
    "        BRONZE_TABLE, \n",
    "        ZEROBUS_URL.split(\".\")[0], \n",
    "        WORKSPACE_URL,\n",
    "        CLIENT_ID, \n",
    "        CLIENT_SECRET))\n",
    "\n",
    "stream = await sdk_handle.create_stream(table_properties, options)\n",
    "\n",
    "if stream:\n",
    "    print(\"Connection successful\")\n",
    "    stream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "217e1cb6-4d15-4c88-bb63-e1f5aa7ace0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Generate sensor data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3049523-09c0-43e1-8377-4b0618e66043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We will use a data generator to simulate data coming from a complex ball bearing production system organized as folllwing. \n",
    "\n",
    "The core elements of this prduction system are: \n",
    "- Production Line\n",
    "  - Machine\n",
    "    - Component\n",
    "      - Sensor\n",
    "\n",
    "<br>\n",
    "<img src=\"./images/ball-bearing-diagram.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "\n",
    "**How does the data generator work**\n",
    "<br>\n",
    "The data genetor will produce data mathing the production line setup you defined in the Digital Twin frontend. The following variables will be used:\n",
    "\n",
    "- Number of lines\n",
    "- Number of machines per line: each line can have a different number of machies.\n",
    "- Number of components per machine: each machine  has the same number of components\n",
    "- Each component has 6 different sensors (fixed) generating data:\n",
    "  - Temperature\n",
    "  - Pressure\n",
    "  - Vibration\n",
    "  - Speed\n",
    "  - Rotation\n",
    "  - Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa45ffe-725e-4c08-aa1a-a9903a397283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from line_data_generator import generate_all_lines, generate_equipment_mapping, table_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c443948-3ae9-4e8a-9924-98708d34866c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Production Line configuration -- TODO: Read from config file\n",
    "num_lines = 4\n",
    "machines_per_line = [3, 3, 4, 2]  # Number of machines per line\n",
    "num_components = 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb8c9e6-d5b0-4498-9131-c569fed54b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<br>You can adjust the size of the generated dataset by setting the sample_size parameter.\n",
    "\n",
    "For instance, if you choose a sample size of 1000, the data generator will produce a dataset with these characteristics:\n",
    "\n",
    "- Each row represents a specific component at a specific timestamp, with component_id and timestamp serving as unique identifiers.\n",
    "- Each component will have 1000 rows, corresponding to the sample size.\n",
    "- Consecutive rows for a given component are spaced 1 millisecond apart. Thus, for a sample size of 1000, the total duration covered per component is 1 second (1000 * 0.001).\n",
    "- The total number of rows in the dataset depends on the number of components in your production line. For example, with 36 components (4 lines, 12 machines, 3 components per machine), the dataset will have 36,000 rows (36 * 1000).\n",
    "- The overall time span for the dataset remains 1 seconds (1000 * 0.001), meaning that sensor data for different components is generated in parallel at the same timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d60ad8-5360-4ec2-96b0-5ef4951ce3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define sample size\n",
    "sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a029e3a8-abd0-4bf3-89d8-154156accbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc643337-0de3-4ecb-b0bb-b0b1bdc3ce17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate equipment mapping: lines -> machine -> components -> sensors \n",
    "equipment_mapping = generate_equipment_mapping(num_lines, machines_per_line, num_components)\n",
    "\n",
    "# Estimate table size\n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows: {tot_num_rows}\")\n",
    "print(f\"Estimated table size: {est_table_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170ae4a2-99d6-4f4e-b723-b6928a3682ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run data generator and display the first 10 rows\n",
    "batch_df_lines = generate_all_lines(equipment_mapping, sample_size, time.time())\n",
    "display(batch_df_lines.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad2964ef-2c9b-45aa-b576-7afe7abd01ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 4: Ingest data via Zerobus API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95893b07-c5f0-4016-81dc-590ffbcd26e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The Zerobus Python SDK is available in two variations:\n",
    "\n",
    "- **Synchronous (sync)**: A traditional, blocking client that uses standard threading for concurrency. This is the default client.\n",
    "- **Asynchronous (async)**: A non-blocking client based on Python's asyncio library, suitable for high-throughput I/O-bound applications.\n",
    "\n",
    "In this notebook we will use Asynchronous non-blocking clients. With the asynchronous client, _ingest_record_ is an async method that returns an asyncio.Future. You can await this _future_ to block until the record is durable.\n",
    "\n",
    "Do not edit the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3179fae-a4f1-4aeb-a154-690de83e6b6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb8401fa-4fd2-4629-9ee0-76f625a0c3e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "A data producer will first open a “stream” to a Delta table, construct a message matching its schema, and then push the message to our API. Zerobus will make the data durable, acknowledge the client's message, and materialize in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d7daa2-e0f7-4fd4-822a-14aaf5199564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def asynchronous_non_blocking_call():\n",
    "\n",
    "    # Create stream to table\n",
    "    stream = await sdk_handle.create_stream(table_properties, options)\n",
    "\n",
    "    # Config progress bar\n",
    "    total = len(batch_df_lines)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=\"Ingesting sensor data via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    # non-blocking (streaming) call to Zerobus \n",
    "    for i in range(total):\n",
    "        \n",
    "        row = batch_df_lines.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "\n",
    "    await stream.close()\n",
    "\n",
    "    # Add ingest info\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Rows ingested: {total}\")\n",
    "    print(f\"Time elapsed: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eafe1a7-c664-462f-95d8-9d7452520496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the client\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await asynchronous_non_blocking_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2deca82a-38a4-4191-8ee4-551d6b506743",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display destination table\n",
    "display(spark.table(BRONZE_TABLE).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493d952b-70f5-4649-ba37-1bb35b0aa4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 5: Simulate data ingestion from  multiple sources in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033accb8-546b-4716-bee1-8a3e56560dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section we will simulate data ingestion from multiple production lines in parallel. Zerobus Ingest will process each production line separately and push the data in the same Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3024109d-39bb-48ad-803a-d0b85d558a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from line_data_generator import generate_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bb3cbbf-305d-46a1-923a-a93ddd262617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define sample size for each production line batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf79e58-95b3-4160-8281-ae6f193b71f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample size for a component in each line\n",
    "line_sample_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa6ec8f-a5e4-4ca2-a3c3-638f00ea226f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b6df702-3ea2-49e3-86d2-588df4144dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, line_sample_size)\n",
    "\n",
    "for line in range(len(line_num_rows)): \n",
    "  print(f\"Line {line+1}\")               \n",
    "  print(f\" - Number of rows: {line_num_rows[line]}\")\n",
    "  print(f\" - Estimated table size: {est_line_table_size[line]:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227c4518-935b-4926-b034-d6078837b381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "The strategy to ingest data from multiple sources in parallel depends on whether you are using the sync or async client. <br> The asynchronous SDK uses an event loop on a single thread. To achieve parallelism, you create multiple asyncio tasks and run them concurrently using asyncio.gather. This is highly efficient for I/O-bound operations.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f50d2ab-5179-44f0-92c1-dc579ab7f212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def worker(worker_id: int):\n",
    "    \"\"\"An async worker that creates a stream and ingests records.\"\"\"\n",
    "    print(f\"Worker {worker_id}: Starting...\")\n",
    "\n",
    "    stream = await sdk_handle.create_stream(table_properties, options)\n",
    "\n",
    "    line_number = worker_id\n",
    "    current_time = time.time()\n",
    "    batch_df_line = generate_line(line_number, equipment_mapping, line_sample_size, current_time)\n",
    "\n",
    "    total = len(batch_df_line)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=f\"Ingesting sensor data for Line {line_number+1} via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(total):\n",
    "\n",
    "        row = batch_df_line.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data for Line {line_number+1} via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "    await stream.close()\n",
    "    print(f\"Worker {worker_id}: Finished.\")\n",
    "\n",
    "\n",
    "async def run_parallel():\n",
    "    num_workers = num_lines\n",
    "    tasks = []\n",
    "    for i in range(num_workers):\n",
    "        task = asyncio.create_task(worker(i))\n",
    "        tasks.append(task)\n",
    "       \n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07785b57-1e2b-49dc-866f-523bf31eeb01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the client\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await run_parallel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ee467c-9bb6-4d39-8429-7205bae28236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 6: Simulate Continuous Data Ingestion from  one source\n",
    "\n",
    "In this section, we will simulate a continuous data ingestion. To prevent creating a large dataset in a single data generation job, we will run multiple data ingestion batches. Each batch will be generated separately and sent to the Delta Table one at a time using Zerobus Ingest. <br>\n",
    "<br>\n",
    "You can configuare the batch sample size and batch count to simulate the desired contunous data ingestion duration.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b24ff529-778e-4703-963f-87525722b40a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Define batch configuration\n",
    "\n",
    "sample_size = 10000   # for each batch\n",
    "batch_count = 10   # number of batches to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a357ede-7627-49ad-885e-6c0f144d14c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With this configuration you will generate a dataset with the following size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af96d931-0c24-4950-a485-a87c415ffaa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total number of rows to be generated \n",
    "tot_num_rows, est_table_size, line_num_rows, est_line_table_size = table_size_estimator(machines_per_line, num_components, sample_size)\n",
    "\n",
    "print(f\"Number of rows in each batch: {tot_num_rows}\")\n",
    "print(f\"Estimated table size in each batch: {est_table_size:.2f} MB\")\n",
    "print(f\"Number of rows for the total dataset: {tot_num_rows * batch_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1924a473-83bd-4d2b-898c-e1667c36fd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**How does the client work** <br>\n",
    "We will use the same clinet configuration used in Step 4.\n",
    "For each batch,  a data producer will first open a “stream” to a Delta table, construct a message matching its schema, and then push the message to our API. Zerobus will make the data durable, acknowledge the client's message, and materialize in the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6994e2-c66e-4288-9631-42240e7e9978",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def batch_asynchronous_blocking_call():\n",
    "\n",
    "  # Create stream to table\n",
    "  stream = await sdk_handle.create_stream(table_properties, options)\n",
    "\n",
    "  min_batch_wait =  sample_size *  0.001   # minumun seconds wait between writes to avoid having overlapping time between batches\n",
    "\n",
    "  for i in range(0, int(batch_count)):\n",
    "    \n",
    "    current_time = time.time()\n",
    "\n",
    "    if i > 0:\n",
    "      if current_time <= batch_time + min_batch_wait:\n",
    "        wait = int(batch_time + min_batch_wait - current_time + 10) # add 10 seconds just in case\n",
    "        time.sleep(wait)\n",
    "        print(f\"Pausing {wait} seconds to avoid overlapping timestamps across batches\")\n",
    "\n",
    "    print(f\"--- Ingesting batch {int(i+1)} / {int(batch_count)} ---\")\n",
    "\n",
    "    # Generate new data\n",
    "    batch_time = time.time()\n",
    "    batch_df_lines = generate_all_lines(equipment_mapping, sample_size, batch_time)\n",
    "\n",
    "    # Config progress bar\n",
    "    total = len(batch_df_lines)\n",
    "    pbar = IntProgress(min=0, max=total)\n",
    "    label = HTML(value=\"Ingesting sensor data via Zerobus Ingest: 0%\")\n",
    "    box = VBox([label, pbar])\n",
    "    display(box)\n",
    "    start = time.time()\n",
    "\n",
    "    # non-blocking (streaming) call. \n",
    "    for i in range(total):\n",
    "        \n",
    "        row = batch_df_lines.iloc[i]\n",
    "\n",
    "        await stream.ingest_record(row_pb2.DigitalTwin(\n",
    "            sensor_rotation=float(row['sensor_rotation']),\n",
    "            sensor_flow=float(row['sensor_flow']),\n",
    "            sensor_temperature=float(row['sensor_temperature']),\n",
    "            sensor_speed=float(row['sensor_speed']),\n",
    "            sensor_vibration=float(row['sensor_vibration']),\n",
    "            sensor_pressure=float(row['sensor_pressure']),\n",
    "            component_yield_output=float(row['component_yield_output']),\n",
    "            timestamp=str(row['timestamp']),\n",
    "            component_id=str(row['component_id']),\n",
    "            damaged_component=bool(row['damaged_component']),\n",
    "            abnormal_sensor=str(row['abnormal_sensor']) if row['abnormal_sensor'] is not None else \"\",\n",
    "            machine_id=str(row['machine_id']),\n",
    "            line_id=str(row['line_id'])\n",
    "        ))\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.value = i + 1\n",
    "        percent = int((i + 1) / total * 100)\n",
    "        label.value = f\"Ingesting sensor data via Zerobus Ingest: {percent}%\"\n",
    "\n",
    "    await stream.flush()\n",
    "\n",
    "    # Add ingest info\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Rows ingested: {total}\")\n",
    "    print(f\"Time elapsed: {elapsed:.2f} seconds\")\n",
    "    print(\"\\n\" * 1)\n",
    "\n",
    "  await stream.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13c6241-2a14-4398-bac1-5680e6891928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the asynchronous function\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "await batch_asynchronous_blocking_call()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "2-Ingest-Data-Zerobus",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}